{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Customer Segmentation Analysis\n",
        "# Using Mall Customers Dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set the visual style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "\"\"\"Load and perform initial exploration of the dataset\"\"\"\n",
        "def load_and_explore_data(filepath):\n",
        "\n",
        "    print(\"Step 1: Loading and exploring data...\")\n",
        "\n",
        "    # Load the dataset\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {filepath} not found.\")\n",
        "\n",
        "    # Display basic information\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "    print(\"\\nData sample:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nData info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    print(\"\\nSummary statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    print(\"\\nMissing values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    print(\"\\nDuplicate rows:\")\n",
        "    print(df.duplicated().sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "\"\"\"Clean, transform and prepare data for analysis\"\"\"\n",
        "def preprocess_data(df):\n",
        "\n",
        "    print(\"\\nStep 2: Preprocessing data...\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    processed_df = df.copy()\n",
        "\n",
        "    # Drop CustomerID as it's not useful for clustering\n",
        "    if 'CustomerID' in processed_df.columns:\n",
        "        processed_df.drop('CustomerID', axis=1, inplace=True)\n",
        "        print(\"Dropped CustomerID column\")\n",
        "\n",
        "    # Handle missing values\n",
        "    # For numerical columns, fill with median\n",
        "    num_cols = processed_df.select_dtypes(include=np.number).columns\n",
        "    for col in num_cols:\n",
        "        if processed_df[col].isnull().sum() > 0:\n",
        "            median_val = processed_df[col].median()\n",
        "            processed_df[col].fillna(median_val, inplace=True)\n",
        "            print(f\"Filled missing values in {col} with median: {median_val}\")\n",
        "\n",
        "    # For categorical columns, fill with mode\n",
        "    cat_cols = processed_df.select_dtypes(exclude=np.number).columns\n",
        "    for col in cat_cols:\n",
        "        if processed_df[col].isnull().sum() > 0:\n",
        "            mode_val = processed_df[col].mode()[0]\n",
        "            processed_df[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"Filled missing values in {col} with mode: {mode_val}\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    before_dedup = len(processed_df)\n",
        "    processed_df.drop_duplicates(inplace=True)\n",
        "    after_dedup = len(processed_df)\n",
        "    print(f\"Removed {before_dedup - after_dedup} duplicate rows\")\n",
        "\n",
        "    # Detect and remove outliers using Z-score\n",
        "    for col in num_cols:\n",
        "        z_scores = stats.zscore(processed_df[col])\n",
        "        abs_z_scores = np.abs(z_scores)\n",
        "        filtered_entries = (abs_z_scores < 3)  # Keep only entries with z-score < 3\n",
        "        outliers = processed_df[~filtered_entries]\n",
        "        if len(outliers) > 0:\n",
        "            print(f\"Removed {len(outliers)} outliers from {col}\")\n",
        "            processed_df = processed_df[filtered_entries]\n",
        "\n",
        "    # print(f\"processed df columns: {processed_df.columns}\")\n",
        "    return processed_df\n",
        "\n",
        "\"\"\"Determine if Gender should be included in clustering and prepare encoded version\"\"\"\n",
        "def analyze_gender_relevance(df):\n",
        "\n",
        "    print(\"\\nStep 3: Analyzing gender relevance...\")\n",
        "\n",
        "    if 'Gender' not in df.columns:\n",
        "        print(\"Gender column not found in dataset\")\n",
        "        return False, df\n",
        "\n",
        "    include_gender = False\n",
        "\n",
        "    # Analyze spending patterns by gender\n",
        "    if 'Spending Score (1-100)' in df.columns:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.boxplot(x='Gender', y='Spending Score (1-100)', data=df)\n",
        "        plt.title('Spending Score by Gender')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.boxplot(x='Gender', y='Annual Income (k$)', data=df)\n",
        "        plt.title('Annual Income by Gender')\n",
        "\n",
        "        # Perform t-test to check if the difference is significant\n",
        "        male_spending = df[df['Gender'] == 'Male']['Spending Score (1-100)']\n",
        "        female_spending = df[df['Gender'] == 'Female']['Spending Score (1-100)']\n",
        "\n",
        "        t_stat, p_value = stats.ttest_ind(male_spending, female_spending, equal_var=False)\n",
        "        print(f\"T-test for Spending Score by Gender: t={t_stat:.2f}, p={p_value:.4f}\")\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            print(\"Gender shows significant difference in spending patterns (p < 0.05)\")\n",
        "            include_gender = True\n",
        "        else:\n",
        "            print(\"Gender does not show significant difference in spending patterns (p >= 0.05)\")\n",
        "\n",
        "        # Also test income differences\n",
        "        male_income = df[df['Gender'] == 'Male']['Annual Income (k$)']\n",
        "        female_income = df[df['Gender'] == 'Female']['Annual Income (k$)']\n",
        "\n",
        "        t_stat, p_value = stats.ttest_ind(male_income, female_income, equal_var=False)\n",
        "        print(f\"T-test for Annual Income by Gender: t={t_stat:.2f}, p={p_value:.4f}\")\n",
        "\n",
        "        if p_value < 0.05:\n",
        "            print(\"Gender shows significant difference in income (p < 0.05)\")\n",
        "            include_gender = True\n",
        "\n",
        "    # Create copies of the dataset\n",
        "    df_with_gender = df.copy()\n",
        "    df_without_gender = df.drop('Gender', axis=1)\n",
        "\n",
        "    # Encode Gender in the df_with_gender dataset\n",
        "    if include_gender:\n",
        "        le = LabelEncoder()\n",
        "        df_with_gender['Gender_Encoded'] = le.fit_transform(df_with_gender['Gender'])\n",
        "        print(f\"Gender encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "    # Return the appropriate dataset based on analysis\n",
        "    if include_gender:\n",
        "        print(\"Decision: Include Gender in the clustering analysis\")\n",
        "        print(include_gender, df_with_gender.columns)\n",
        "        return True, df_with_gender\n",
        "    else:\n",
        "        print(\"Decision: Exclude Gender from the clustering analysis\")\n",
        "        print(include_gender, df_without_gender.columns)\n",
        "        return False, df_without_gender\n",
        "\n",
        "\"\"\"Standardize numerical features to ensure equal contribution\"\"\"\n",
        "def standardize_features(df):\n",
        "\n",
        "    print(\"\\nStep 4: Standardizing features...\")\n",
        "\n",
        "    # Identify numerical columns (excluding any encoded categories we want to keep separate)\n",
        "    num_cols = [col for col in df.select_dtypes(include=np.number).columns\n",
        "                if not col.endswith('_Encoded')]\n",
        "\n",
        "    print(f\"Standardizing columns: {num_cols}\")\n",
        "\n",
        "    # Create a copy of the dataframe\n",
        "    scaled_df = df.copy()\n",
        "\n",
        "    # Apply standardization\n",
        "    scaler = StandardScaler()\n",
        "    scaled_df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "    print(\"Features standardized successfully\")\n",
        "    return scaled_df, scaler\n",
        "\n",
        "\"\"\"Perform EDA to visualize data distributions and relationships\"\"\"\n",
        "def exploratory_data_analysis(df, original_df):\n",
        "\n",
        "    print(\"\\nStep 5: Exploratory Data Analysis...\")\n",
        "\n",
        "    # 1. Distribution of numerical features\n",
        "    num_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(num_cols):\n",
        "        if '_Encoded' not in col:  # Skip encoded categorical values\n",
        "            plt.subplot(1, len(num_cols), i+1)\n",
        "            sns.histplot(original_df[col], kde=True)\n",
        "            plt.title(f'Distribution of {col}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 2. Correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    correlation_matrix = df.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "\n",
        "    # 3. Pairplot for numerical features\n",
        "    if len(num_cols) >= 2:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.pairplot(original_df, vars=[col for col in num_cols if '_Encoded' not in col],\n",
        "                     hue='Gender' if 'Gender' in original_df.columns else None)\n",
        "        plt.suptitle('Pairplot of Features', y=1.02)\n",
        "\n",
        "    return correlation_matrix\n",
        "\n",
        "\"\"\"Apply dimensionality reduction techniques\"\"\"\n",
        "def reduce_dimensions(df):\n",
        "\n",
        "    print(\"\\nStep 6: Dimensionality Reduction...\")\n",
        "\n",
        "    # Extract features for clustering (all numerical columns)\n",
        "    features = df.select_dtypes(include=np.number).values\n",
        "\n",
        "    # 1. Principal Component Analysis (PCA)\n",
        "    pca = PCA()\n",
        "    pca_result = pca.fit_transform(features)\n",
        "\n",
        "    # Analyze explained variance\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')\n",
        "    plt.title('PCA Explained Variance')\n",
        "    plt.legend()\n",
        "\n",
        "    # Find number of components for 95% variance\n",
        "    n_components_95 = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
        "    print(f\"Number of components needed for 95% variance: {n_components_95}\")\n",
        "\n",
        "    # Keep components for 95% variance\n",
        "    pca = PCA(n_components=n_components_95)\n",
        "    pca_result = pca.fit_transform(features)\n",
        "\n",
        "    # 2. t-SNE for visualization (always produces 2 components for visualization)\n",
        "    if features.shape[0] > 10:  # t-SNE requires more than a few samples\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        tsne_result = tsne.fit_transform(features)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7)\n",
        "        plt.title('t-SNE Visualization of Customer Data')\n",
        "        plt.xlabel('t-SNE Component 1')\n",
        "        plt.ylabel('t-SNE Component 2')\n",
        "    else:\n",
        "        tsne_result = None\n",
        "        print(\"Not enough samples for t-SNE visualization\")\n",
        "\n",
        "    return pca_result, tsne_result, pca\n",
        "\n",
        "\"\"\"Determine the optimal number of clusters using various methods\"\"\"\n",
        "def determine_optimal_clusters(df, pca_result):\n",
        "\n",
        "    print(\"\\nStep 7: Determining optimal number of clusters...\")\n",
        "\n",
        "    # Elbow Method for K-Means\n",
        "    inertia = []\n",
        "    silhouette_scores = []\n",
        "    k_range = range(2, 11)\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        kmeans.fit(pca_result)\n",
        "        inertia.append(kmeans.inertia_)\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        if k > 1:  # Silhouette score requires at least 2 clusters\n",
        "            labels = kmeans.labels_\n",
        "            silhouette_avg = silhouette_score(pca_result, labels)\n",
        "            silhouette_scores.append(silhouette_avg)\n",
        "            print(f\"For k={k}, silhouette score: {silhouette_avg:.3f}\")\n",
        "\n",
        "    # Plot Elbow Method\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(k_range, inertia, marker='o')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.title('K-Means Elbow Method')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(k_range[0:], silhouette_scores, marker='o')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.title('Silhouette Scores')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Find optimal k from silhouette score\n",
        "    optimal_k_silhouette = k_range[0:][np.argmax(silhouette_scores)]\n",
        "    print(f\"Optimal K based on silhouette score: {optimal_k_silhouette}\")\n",
        "\n",
        "    # Hierarchical Clustering - Dendrogram\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    linked = linkage(pca_result, 'ward')\n",
        "    dendrogram(linked, truncate_mode='lastp', p=15)\n",
        "    plt.title('Hierarchical Clustering Dendrogram')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Distance')\n",
        "    optimal_k_dendrogram = 6 #hardcoded after analyzing diagram\n",
        "    print(f\"Optimal K based on dendogram (hardcoded after analyzing graph): {optimal_k_dendrogram}\")\n",
        "\n",
        "    # GMM - AIC and BIC\n",
        "    bic = []\n",
        "    aic = []\n",
        "\n",
        "    for k in k_range:\n",
        "        gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "        gmm.fit(pca_result)\n",
        "        bic.append(gmm.bic(pca_result))\n",
        "        aic.append(gmm.aic(pca_result))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(k_range, bic, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('BIC')\n",
        "    plt.title('BIC for GMM')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(k_range, aic, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('AIC')\n",
        "    plt.title('AIC for GMM')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Find optimal k from BIC\n",
        "    optimal_k_bic = k_range[np.argmin(bic)]\n",
        "    print(f\"Optimal K based on BIC: {optimal_k_bic}\")\n",
        "\n",
        "    # Return different optimal k values from different methods\n",
        "    return {\n",
        "        'kmeans': optimal_k_silhouette,\n",
        "        'gmm': optimal_k_bic,\n",
        "        'dendogram': optimal_k_dendrogram\n",
        "    }\n",
        "\n",
        "\"\"\"Perform clustering using various algorithms\"\"\"\n",
        "def perform_clustering(pca_result, optimal_k, original_df, include_gender):\n",
        "\n",
        "    print(\"\\nStep 8: Performing clustering with optimal K =\", optimal_k)\n",
        "\n",
        "    # K-Means Clustering\n",
        "    kmeans = KMeans(n_clusters=optimal_k['kmeans'], random_state=42, n_init=10)\n",
        "    kmeans_labels = kmeans.fit_predict(pca_result)\n",
        "\n",
        "    # Hierarchical Clustering\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=optimal_k['kmeans'])\n",
        "    hierarchical_labels = hierarchical.fit_predict(pca_result)\n",
        "\n",
        "    # Gaussian Mixture Model\n",
        "    gmm = GaussianMixture(n_components=optimal_k['gmm'], random_state=42)\n",
        "    gmm.fit(pca_result)\n",
        "    gmm_labels = gmm.predict(pca_result)\n",
        "\n",
        "    # Add cluster labels to the original dataframe\n",
        "    clustered_df = original_df.copy()\n",
        "    clustered_df['KMeans_Cluster'] = kmeans_labels\n",
        "    clustered_df['Hierarchical_Cluster'] = hierarchical_labels\n",
        "    clustered_df['GMM_Cluster'] = gmm_labels\n",
        "\n",
        "    # Visualize clusters in 2D PCA space\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # K-Means visualization\n",
        "    plt.subplot(1, 3, 1)\n",
        "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)\n",
        "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "                c='red', marker='X', s=100, label='Centroids')\n",
        "    plt.title('K-Means Clustering')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.legend()\n",
        "\n",
        "    # Hierarchical visualization\n",
        "    plt.subplot(1, 3, 2)\n",
        "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=hierarchical_labels, cmap='viridis', alpha=0.7)\n",
        "    plt.title('Hierarchical Clustering')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # GMM visualization\n",
        "    plt.subplot(1, 3, 3)\n",
        "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=gmm_labels, cmap='viridis', alpha=0.7)\n",
        "    plt.title('Gaussian Mixture Model')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # If we have 3 or more PCA components, create 3D visualization\n",
        "    if pca_result.shape[1] >= 3:\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        scatter = ax.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2],\n",
        "                            c=kmeans_labels, cmap='viridis', alpha=0.7)\n",
        "        ax.set_title('3D K-Means Clustering Visualization')\n",
        "        ax.set_xlabel('PCA Component 1')\n",
        "        ax.set_ylabel('PCA Component 2')\n",
        "        ax.set_zlabel('PCA Component 3')\n",
        "        plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # Compare algorithms\n",
        "    from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "    ari_km_hc = adjusted_rand_score(kmeans_labels, hierarchical_labels)\n",
        "    ari_km_gmm = adjusted_rand_score(kmeans_labels, gmm_labels)\n",
        "    ari_hc_gmm = adjusted_rand_score(hierarchical_labels, gmm_labels)\n",
        "\n",
        "    print(f\"Adjusted Rand Index (KMeans vs Hierarchical): {ari_km_hc:.3f}\")\n",
        "    print(f\"Adjusted Rand Index (KMeans vs GMM): {ari_km_gmm:.3f}\")\n",
        "    print(f\"Adjusted Rand Index (Hierarchical vs GMM): {ari_hc_gmm:.3f}\")\n",
        "\n",
        "    # Choose the best algorithm based on silhouette score\n",
        "    silhouette_km = silhouette_score(pca_result, kmeans_labels)\n",
        "    silhouette_hc = silhouette_score(pca_result, hierarchical_labels)\n",
        "    silhouette_gmm = silhouette_score(pca_result, gmm_labels)\n",
        "\n",
        "    print(f\"K-Means Silhouette Score: {silhouette_km:.3f}\")\n",
        "    print(f\"Hierarchical Silhouette Score: {silhouette_hc:.3f}\")\n",
        "    print(f\"GMM Silhouette Score: {silhouette_gmm:.3f}\")\n",
        "\n",
        "    scores = {\n",
        "        'KMeans': silhouette_km,\n",
        "        'Hierarchical': silhouette_hc,\n",
        "        'GMM': silhouette_gmm\n",
        "    }\n",
        "    best_algorithm = max(scores, key=scores.get)\n",
        "    print(f\"Best clustering algorithm based on silhouette score: {best_algorithm}\")\n",
        "\n",
        "    # Select the best algorithm's labels\n",
        "    if best_algorithm == 'KMeans':\n",
        "        final_labels = kmeans_labels\n",
        "        clustered_df['Cluster'] = kmeans_labels\n",
        "        model = kmeans\n",
        "    elif best_algorithm == 'Hierarchical':\n",
        "        final_labels = hierarchical_labels\n",
        "        clustered_df['Cluster'] = hierarchical_labels\n",
        "        model = hierarchical\n",
        "    else:  # GMM\n",
        "        final_labels = gmm_labels\n",
        "        clustered_df['Cluster'] = gmm_labels\n",
        "        model = gmm\n",
        "\n",
        "    return clustered_df, final_labels, model, best_algorithm\n",
        "\n",
        "\"\"\"Profile and interpret each cluster\"\"\"\n",
        "def profile_clusters(clustered_df, include_gender):\n",
        "\n",
        "    print(\"\\nStep 9: Profiling clusters...\")\n",
        "\n",
        "    # Focus on the final cluster assignment\n",
        "    cluster_profiles = clustered_df.groupby('Cluster').agg({\n",
        "        'Age': ['mean', 'min', 'max'],\n",
        "        'Annual Income (k$)': ['mean', 'min', 'max'],\n",
        "        'Spending Score (1-100)': ['mean', 'min', 'max']\n",
        "    })\n",
        "\n",
        "    print(\"\\nCluster Profiles:\")\n",
        "    print(cluster_profiles)\n",
        "\n",
        "    # Calculate gender distribution if gender was included\n",
        "    if include_gender and 'Gender' in clustered_df.columns:\n",
        "        gender_distribution = pd.crosstab(\n",
        "            clustered_df['Cluster'],\n",
        "            clustered_df['Gender'],\n",
        "            normalize='index'\n",
        "        ) * 100  # Convert to percentage\n",
        "\n",
        "        print(\"\\nGender Distribution by Cluster (%):\")\n",
        "        print(gender_distribution)\n",
        "\n",
        "    # Visualize the cluster profiles\n",
        "    # 1. Age distribution by cluster\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.boxplot(x='Cluster', y='Age', data=clustered_df)\n",
        "    plt.title('Age Distribution by Cluster')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.boxplot(x='Cluster', y='Annual Income (k$)', data=clustered_df)\n",
        "    plt.title('Income Distribution by Cluster')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.boxplot(x='Cluster', y='Spending Score (1-100)', data=clustered_df)\n",
        "    plt.title('Spending Score Distribution by Cluster')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 2. 3D Scatter plot of clusters\n",
        "    fig = plt.figure(figsize=(12, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    scatter = ax.scatter(\n",
        "        clustered_df['Annual Income (k$)'],\n",
        "        clustered_df['Spending Score (1-100)'],\n",
        "        clustered_df['Age'],\n",
        "        c=clustered_df['Cluster'],\n",
        "        cmap='viridis',\n",
        "        s=50,\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel('Annual Income (k$)')\n",
        "    ax.set_ylabel('Spending Score (1-100)')\n",
        "    ax.set_zlabel('Age')\n",
        "    ax.set_title('3D Visualization of Customer Clusters')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "\n",
        "    # Create personas for each cluster\n",
        "    n_clusters = clustered_df['Cluster'].nunique()\n",
        "    personas = {}\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        cluster_data = clustered_df[clustered_df['Cluster'] == cluster]\n",
        "\n",
        "        avg_age = cluster_data['Age'].mean()\n",
        "        avg_income = cluster_data['Annual Income (k$)'].mean()\n",
        "        avg_spending = cluster_data['Spending Score (1-100)'].mean()\n",
        "\n",
        "        # Determine persona characteristics\n",
        "        # Age category\n",
        "        if avg_age < 30:\n",
        "            age_category = \"Young\"\n",
        "        elif avg_age < 50:\n",
        "            age_category = \"Middle-aged\"\n",
        "        else:\n",
        "            age_category = \"Senior\"\n",
        "\n",
        "        # Income category\n",
        "        if avg_income < 40:\n",
        "            income_category = \"Budget-conscious\"\n",
        "        elif avg_income < 70:\n",
        "            income_category = \"Middle-income\"\n",
        "        else:\n",
        "            income_category = \"Affluent\"\n",
        "\n",
        "        # Spending category\n",
        "        if avg_spending < 40:\n",
        "            spending_category = \"Conservative Spenders\"\n",
        "        elif avg_spending < 70:\n",
        "            spending_category = \"Moderate Spenders\"\n",
        "        else:\n",
        "            spending_category = \"Big Spenders\"\n",
        "\n",
        "        # Gender majority if applicable\n",
        "        gender_majority = \"\"\n",
        "        if include_gender and 'Gender' in clustered_df.columns:\n",
        "            gender_counts = cluster_data['Gender'].value_counts(normalize=True)\n",
        "            if len(gender_counts) > 0:\n",
        "                majority_gender = gender_counts.idxmax()\n",
        "                if gender_counts[majority_gender] > 0.65:  # If more than 65% are one gender\n",
        "                    gender_majority = f\"Predominantly {majority_gender}\"\n",
        "\n",
        "        # Create persona name and description\n",
        "        persona_name = f\"{age_category} {income_category} {spending_category}\"\n",
        "        if gender_majority:\n",
        "            persona_name = f\"{gender_majority} {persona_name}\"\n",
        "\n",
        "        # Business recommendations\n",
        "        if avg_spending > 70 and avg_income > 70:\n",
        "            recommendation = \"Target with luxury products and premium services\"\n",
        "        elif avg_spending > 70 and avg_income < 70:\n",
        "            recommendation = \"Target with 'affordable luxury' and exclusive deals\"\n",
        "        elif avg_spending < 40 and avg_income > 70:\n",
        "            recommendation = \"Target with value propositions emphasizing quality and longevity\"\n",
        "        elif avg_age < 30 and avg_spending > 50:\n",
        "            recommendation = \"Target with trendy products and digital marketing campaigns\"\n",
        "        elif avg_age > 50:\n",
        "            recommendation = \"Target with comfort and reliability-focused marketing\"\n",
        "        else:\n",
        "            recommendation = \"Target with balanced value-for-money offerings\"\n",
        "\n",
        "        personas[cluster] = {\n",
        "            'name': persona_name,\n",
        "            'avg_age': avg_age,\n",
        "            'avg_income': avg_income,\n",
        "            'avg_spending': avg_spending,\n",
        "            'recommendation': recommendation\n",
        "        }\n",
        "\n",
        "    print(\"\\nCustomer Personas:\")\n",
        "    for cluster, persona in personas.items():\n",
        "        print(f\"\\nCluster {cluster}: {persona['name']}\")\n",
        "        print(f\"  Average Age: {persona['avg_age']:.1f}\")\n",
        "        print(f\"  Average Income: ${persona['avg_income']:.1f}k\")\n",
        "        print(f\"  Average Spending Score: {persona['avg_spending']:.1f}/100\")\n",
        "        print(f\"  Marketing Recommendation: {persona['recommendation']}\")\n",
        "\n",
        "    return personas\n",
        "\n",
        "\"\"\"Build a model for predicting clusters for new customers\"\"\"\n",
        "def build_predictive_model(preprocessed_df, pca, model, best_algorithm, scaler, include_gender):\n",
        "\n",
        "    print(\"\\nStep 10: Building predictive model...\")\n",
        "\n",
        "    # Identify the exact columns used for fitting the PCA\n",
        "    # These must match between training and prediction\n",
        "    pca_features = [col for col in preprocessed_df.columns\n",
        "                   if col not in ['Cluster', 'KMeans_Cluster', 'Hierarchical_Cluster', 'GMM_Cluster', 'Gender']]\n",
        "\n",
        "    print(f\"PCA was trained on these features: {pca_features}\")\n",
        "    print(f\"Number of features used in PCA: {len(pca_features)}\")\n",
        "\n",
        "    def predict_cluster(new_customer_data, include_gender=False):\n",
        "        \"\"\"\n",
        "        Predict the cluster for a new customer\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        new_customer_data : dict\n",
        "            Dictionary containing customer data with keys matching the original dataset\n",
        "        include_gender : bool\n",
        "            Whether gender should be included in prediction\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int : Predicted cluster\n",
        "        \"\"\"\n",
        "        # Convert dictionary to DataFrame\n",
        "        new_df = pd.DataFrame([new_customer_data])\n",
        "\n",
        "        # Make sure we have all required features\n",
        "        if 'Gender' in new_df.columns and include_gender:\n",
        "            # Handle gender encoding consistently\n",
        "            le = LabelEncoder()\n",
        "            le.fit(['Male', 'Female'])\n",
        "            new_df['Gender_Encoded'] = le.transform(new_df['Gender'])\n",
        "\n",
        "        # Verify that all required features exist\n",
        "        for feature in pca_features:\n",
        "            if feature not in new_df.columns:\n",
        "                raise ValueError(f\"Missing feature: {feature}. Required features: {pca_features}\")\n",
        "\n",
        "        # Select exactly the same features in the same order\n",
        "        X_predict = new_df[pca_features].values\n",
        "\n",
        "        # Apply the same preprocessing\n",
        "        X_scaled = scaler.transform(X_predict)\n",
        "\n",
        "        # Apply PCA transformation\n",
        "        X_pca = pca.transform(X_scaled)\n",
        "\n",
        "        # Make prediction\n",
        "        if best_algorithm == 'KMeans':\n",
        "            predicted_cluster = model.predict(X_pca)[0]\n",
        "        elif best_algorithm == 'Hierarchical':\n",
        "            # For hierarchical clustering, find the nearest cluster\n",
        "            features_array = preprocessed_df[pca_features].values\n",
        "            X_all = np.vstack([pca.transform(scaler.transform(features_array)), X_pca])\n",
        "            predicted_cluster = model.fit_predict(X_all)[-1]\n",
        "        else:  # GMM\n",
        "            predicted_cluster = model.predict(X_pca)[0]\n",
        "\n",
        "        return predicted_cluster\n",
        "\n",
        "    print(\"Model successfully built for inference\")\n",
        "    print(f\"Using {best_algorithm} algorithm for predictions\")\n",
        "\n",
        "    # Example usage\n",
        "    example_customer = {\n",
        "        'Age': 30,\n",
        "        'Annual Income (k$)': 60,\n",
        "        'Spending Score (1-100)': 75\n",
        "    }\n",
        "\n",
        "    if include_gender:\n",
        "        example_customer['Gender'] = 'Female'\n",
        "        if 'Gender_Encoded' in pca_features:\n",
        "            # We need to perform the encoding here too since it's part of our features\n",
        "            le = LabelEncoder()\n",
        "            le.fit(['Male', 'Female'])\n",
        "            example_customer['Gender_Encoded'] = le.transform([example_customer['Gender']])[0]\n",
        "\n",
        "    # Make sure all features from PCA are present in the example\n",
        "    for feature in pca_features:\n",
        "        if feature not in example_customer and feature != 'Gender_Encoded':\n",
        "            print(f\"Warning: Adding missing feature {feature} with value 0\")\n",
        "            example_customer[feature] = 0\n",
        "\n",
        "    example_cluster = predict_cluster(example_customer, include_gender)\n",
        "    print(f\"\\nExample prediction:\")\n",
        "    print(f\"Customer data: {example_customer}\")\n",
        "    print(f\"Predicted cluster: {example_cluster}\")\n",
        "\n",
        "    return predict_cluster\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the complete customer segmentation pipeline\"\"\"\n",
        "    print(\"========= CUSTOMER SEGMENTATION ANALYSIS =========\\n\")\n",
        "\n",
        "    # 1. Load and explore data\n",
        "    df = load_and_explore_data('Mall_Customers.csv')\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    processed_df = preprocess_data(df)\n",
        "\n",
        "    # 3. Analyze gender relevance\n",
        "    include_gender, df_for_analysis = analyze_gender_relevance(processed_df)\n",
        "\n",
        "    # 4. Standardize features\n",
        "    scaled_df, scaler = standardize_features(df_for_analysis)\n",
        "\n",
        "    # 5. Perform EDA\n",
        "    correlation_matrix = exploratory_data_analysis(scaled_df, df_for_analysis)\n",
        "\n",
        "    # 6. Reduce dimensions\n",
        "    pca_result, tsne_result, pca = reduce_dimensions(scaled_df)\n",
        "\n",
        "    # 7. Determine optimal clusters\n",
        "    optimal_k = determine_optimal_clusters(scaled_df, pca_result)\n",
        "\n",
        "    # 8. Perform clustering\n",
        "    clustered_df, final_labels, model, best_algorithm = perform_clustering(\n",
        "        pca_result, optimal_k, df_for_analysis, include_gender)\n",
        "\n",
        "    # 9. Profile clusters\n",
        "    personas = profile_clusters(clustered_df, include_gender)\n",
        "\n",
        "    # 10. Build predictive model\n",
        "    predict_cluster = build_predictive_model(\n",
        "        scaled_df, pca, model, best_algorithm, scaler, include_gender\n",
        "    )\n",
        "\n",
        "    print(\"\\n========= ANALYSIS COMPLETE =========\")\n",
        "    print(f\"Best clustering algorithm: {best_algorithm}\")\n",
        "    print(f\"Number of clusters identified: {len(personas)}\")\n",
        "    print(\"Instructions:\\n\")\n",
        "    print(\"1. Use the predictive model to classify new customers\")\n",
        "    print(\"2. Implement recommended targeted marketing strategies based on cluster personas\")\n",
        "    print(\"3. Re-train the model periodically as new customer data becomes available\")\n",
        "\n",
        "    return clustered_df, personas, predict_cluster\n",
        "\n",
        "def retrain_model(original_df, new_customers_df, frequency=100):\n",
        "    \"\"\"\n",
        "    Retrain the model after adding a certain number of new customers\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    original_df : DataFrame\n",
        "        Original customer dataset\n",
        "    new_customers_df : DataFrame\n",
        "        New customer data to be added\n",
        "    frequency : int\n",
        "        Number of customers to add before retraining\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : Updated dataframe, personas, and prediction function\n",
        "    \"\"\"\n",
        "    print(f\"\\nRetraining model after adding {len(new_customers_df)} new customers...\")\n",
        "\n",
        "    # Combine original and new data\n",
        "    combined_df = pd.concat([original_df, new_customers_df], ignore_index=True)\n",
        "\n",
        "    # Run the entire pipeline again\n",
        "    # This is simplified - in production, you might want to save parameters and models\n",
        "    clustered_df, personas, predict_cluster = main()\n",
        "\n",
        "    print(\"Model retrained successfully\")\n",
        "\n",
        "    return clustered_df, personas, predict_cluster\n",
        "\n",
        "# Run the pipeline if script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    clustered_df, personas, predict_cluster = main()\n",
        "\n",
        "    # Example of how to use the prediction function\n",
        "    new_customer = {\n",
        "        'Age': 42,\n",
        "        'Annual Income (k$)': 85,\n",
        "        'Spending Score (1-100)': 55,\n",
        "        'Gender': 'Male'\n",
        "    }\n",
        "\n",
        "    # Predict cluster for the new customer\n",
        "    predicted_cluster = predict_cluster(new_customer)\n",
        "\n",
        "    print(f\"\\nNew customer details: {new_customer}\")\n",
        "    print(f\"Predicted cluster: {predicted_cluster}\")\n",
        "    print(f\"Customer persona: {personas[predicted_cluster]['name']}\")\n",
        "    print(f\"Recommended marketing approach: {personas[predicted_cluster]['recommendation']}\")"
      ],
      "metadata": {
        "id": "RNhngXV9_RHQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "a461de75-00ed-490e-feaf-9b05bf001252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= CUSTOMER SEGMENTATION ANALYSIS =========\n",
            "\n",
            "Step 1: Loading and exploring data...\n",
            "File Mall_Customers.csv not found.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'df' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-22a28102f260>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;31m# Run the pipeline if script is executed directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m     \u001b[0mclustered_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersonas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;31m# Example of how to use the prediction function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-22a28102f260>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;31m# 1. Load and explore data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_explore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mall_Customers.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;31m# 2. Preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-22a28102f260>\u001b[0m in \u001b[0;36mload_and_explore_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Display basic information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nData sample:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'df' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSGLdYhGBIUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}